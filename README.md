# LLM-All-Explorations-Notebook
# What is generative AI?
https://www.mckinsey.com/featured-insights/mckinsey-explainers/what-is-generative-ai
# Advanced RAG on HuggingFace documentation using LangChain
https://huggingface.co/learn/cookbook/advanced_rag
# RAG Evaluation
https://huggingface.co/learn/cookbook/rag_evaluation
# References
https://github.com/huggingface/notebooks/blob/main/examples/summarization.ipynb
# Understanding Parameter-Efficient LLM Finetuning: Prompt Tuning And Prefix Tuning
https://magazine.sebastianraschka.com/p/understanding-parameter-efficient
# Fine Tuning vs. Prompt Engineering Large Language Models
https://www.union.ai/blog-post/fine-tuning-vs-prompt-tuning-large-language-models
# Prefix-Tuning: Optimizing Continuous Prompts for Generation
https://www.youtube.com/watch?v=TwE2m6Z991s
# ROUGE and BLEU scores for NLP model evaluation
https://clementbm.github.io/theory/2021/12/23/rouge-bleu-scores.html
# Evaluating Large Language Models Trained on Code
https://arxiv.org/abs/2107.03374
#Understanding BLEU and ROUGE score for NLP evaluation
https://medium.com/@sthanikamsanthosh1994/understanding-bleu-and-rouge-score-for-nlp-evaluation-1ab334ecadcb
# METEOR metric for machine translation
https://machinelearninginterview.com/topics/machine-learning/meteor-for-machine-translation/
# Training language models to follow instructions with human feedback
https://arxiv.org/abs/2203.02155
# Illustrating Reinforcement Learning from Human Feedback (RLHF)
https://huggingface.co/blog/rlhf
# Hugging face all use case
https://github.com/huggingface/transformers/tree/main/examples/pytorch
# list of the official notebooks provided by Hugging Face.
https://github.com/huggingface/transformers/tree/main/notebooks#pytorch-examples
# How to generate text: using different decoding methods for language generation with Transformers
https://github.com/huggingface/blog/blob/main/notebooks/02_how_to_generate.ipynb
# A Full Guide to Finetuning T5 for Text2Text and Building a Demo with Streamlit
https://medium.com/nlplanet/a-full-guide-to-finetuning-t5-for-text2text-and-building-a-demo-with-streamlit-c72009631887
# How to train a new language model from scratch using Transformers and Tokenizers
https://huggingface.co/blog/how-to-train
# Fine-tuning a masked language model
https://huggingface.co/learn/nlp-course/chapter7/3?fw=pt
# examples/pytorch/language-modeling
https://github.com/huggingface/transformers/tree/main/examples/pytorch/language-modeling
# Generative Pre-training (GPT) for Natural Language Understanding
https://www.analyticsvidhya.com/blog/2022/10/generative-pre-training-gpt-for-natural-language-understanding/
# Attention is All You Need
https://research.google/pubs/attention-is-all-you-need/
# Exploring Language Models for Generative AI: An Introduction to LLMs
https://soulpageit.com/exploring-language-models-for-generative-ai-an-introduction-to-llms/
# Introduction to Large Language Models and the Transformer Architecture
https://rpradeepmenon.medium.com/introduction-to-large-language-models-and-the-transformer-architecture-534408ed7e61
# Transformer, GPT-3,GPT-J, T5 and BERT.
https://aliissa99.medium.com/transformer-gpt-3-gpt-j-t5-and-bert-4cf8915dd86f
# The Illustrated Transformer
https://jalammar.github.io/illustrated-transformer/
# https://www.researchgate.net/figure/A-schematic-comparison-of-BART-with-BERT-and-GPT-72-input-transformation-methods_fig2_351095430
# The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)
https://jalammar.github.io/illustrated-bert/
# https://www.youtube.com/@ChrisMcCormickAI
#Language Models are Few-Shot Learners
https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf

